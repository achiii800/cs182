{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muon Experiments Notebook\n",
    "\n",
    "**EECS 182 Final Project - Running Experiments**\n",
    "\n",
    "This notebook runs the core experiments for our project on spectral-norm constrained optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiments Overview\n",
    "\n",
    "1. **Baseline Comparison**: SGD vs AdamW vs MuonSGD on SmallCNN\n",
    "2. **Inner Solver Comparison**: Different solvers on ResNet-18\n",
    "3. **Spectral Budget Sweep**: Effect of budget on training dynamics\n",
    "4. **Width Transfer Experiment**: muP-style width scaling\n",
    "5. **LR Stability Envelope**: Finding max stable learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "os.chdir('..')  # Change to project root\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Our modules\n",
    "from muon import (\n",
    "    MuonSGD, MuonAdamW, create_optimizer,\n",
    "    SpectralClipSolver, FrankWolfeSolver, DualAscentSolver,\n",
    "    QuasiNewtonDualSolver, ADMMSolver, get_inner_solver,\n",
    "    compute_spectral_norms, estimate_sharpness, estimate_gradient_noise_scale,\n",
    "    MetricsLogger\n",
    ")\n",
    "from models import get_model, MODEL_REGISTRY\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Available models: {list(MODEL_REGISTRY.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading utilities\n",
    "\n",
    "def get_cifar10_loaders(batch_size=128, num_workers=2):\n",
    "    \"\"\"Create CIFAR-10 train/test loaders with standard augmentation.\"\"\"\n",
    "    \n",
    "    transform_train = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    \n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load data\n",
    "train_loader, test_loader = get_cifar10_loaders(batch_size=128)\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "\n",
    "def ce_loss_fn(model, x, y):\n",
    "    \"\"\"Cross-entropy loss for metrics API.\"\"\"\n",
    "    return F.cross_entropy(model(x), y)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model and return (loss, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        total_loss += F.cross_entropy(logits, y, reduction='sum').item()\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, scheduler=None):\n",
    "    \"\"\"Train for one epoch and return (loss, accuracy).\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model_name='small_cnn',\n",
    "    optimizer_type='muon_sgd',\n",
    "    inner_solver_type='spectral_clip',\n",
    "    spectral_budget=0.1,\n",
    "    lr=0.1,\n",
    "    epochs=10,\n",
    "    width_mult=1.0,\n",
    "    seed=42,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"Run a complete training experiment and return metrics history.\"\"\"\n",
    "    \n",
    "    # Set seed\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Create model\n",
    "    model = get_model(model_name, num_classes=10, width_mult=width_mult).to(device)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Model: {model_name}, Params: {num_params:,}, Width: {width_mult}\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = create_optimizer(\n",
    "        model,\n",
    "        optimizer_type=optimizer_type,\n",
    "        inner_solver_type=inner_solver_type,\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=5e-4,\n",
    "        spectral_budget=spectral_budget if inner_solver_type != 'none' else None\n",
    "    )\n",
    "    \n",
    "    # LR scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Metrics logger\n",
    "    logger = MetricsLogger()\n",
    "    \n",
    "    # Get batches for GNS estimation\n",
    "    gns_batches = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        gns_batches.append(batch)\n",
    "        if len(gns_batches) >= 2:\n",
    "            break\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_acc = evaluate(model, test_loader, device)\n",
    "        \n",
    "        # Compute metrics\n",
    "        spec_norms = compute_spectral_norms(model, max_layers=8)\n",
    "        max_spec = max(spec_norms.values()) if spec_norms else 0.0\n",
    "        \n",
    "        sharpness = estimate_sharpness(\n",
    "            model, ce_loss_fn,\n",
    "            gns_batches[0][0], gns_batches[0][1],\n",
    "            epsilon=1e-3\n",
    "        )\n",
    "        \n",
    "        gns = estimate_gradient_noise_scale(\n",
    "            model, ce_loss_fn,\n",
    "            gns_batches[0], gns_batches[1]\n",
    "        )\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_time = time.time() - t0\n",
    "        \n",
    "        # Log\n",
    "        metrics = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'max_spectral_norm': max_spec,\n",
    "            'sharpness': sharpness,\n",
    "            'grad_noise_scale': gns,\n",
    "            'lr': current_lr,\n",
    "            'time': epoch_time\n",
    "        }\n",
    "        logger.log(metrics)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch:3d} | Train: {train_loss:.4f}/{train_acc:.4f} | \"\n",
    "                  f\"Val: {val_loss:.4f}/{val_acc:.4f} | σ_max: {max_spec:.3f}\")\n",
    "    \n",
    "    return logger.history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Baseline Comparison (SmallCNN)\n",
    "\n",
    "Compare SGD, AdamW, and MuonSGD on the small CNN to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Quick baseline comparison\n",
    "EPOCHS = 10  # Quick run; increase for full experiments\n",
    "\n",
    "results_baseline = {}\n",
    "\n",
    "configs = [\n",
    "    ('SGD', 'sgd', 'none', 0.1),\n",
    "    ('MuonSGD + SpectralClip', 'muon_sgd', 'spectral_clip', 0.1),\n",
    "    ('MuonSGD + DualAscent', 'muon_sgd', 'dual_ascent', 0.1),\n",
    "]\n",
    "\n",
    "for name, opt, solver, lr in configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    history, _ = run_experiment(\n",
    "        model_name='small_cnn',\n",
    "        optimizer_type=opt,\n",
    "        inner_solver_type=solver,\n",
    "        spectral_budget=0.1,\n",
    "        lr=lr,\n",
    "        epochs=EPOCHS,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    results_baseline[name] = history\n",
    "\n",
    "print(\"\\nBaseline experiments complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('val_loss', 'Validation Loss'),\n",
    "    ('val_acc', 'Validation Accuracy'),\n",
    "    ('max_spectral_norm', 'Max Spectral Norm'),\n",
    "    ('sharpness', 'Sharpness'),\n",
    "    ('grad_noise_scale', 'Gradient Noise Scale'),\n",
    "    ('train_loss', 'Training Loss'),\n",
    "]\n",
    "\n",
    "for ax, (metric, title) in zip(axes.flatten(), metrics_to_plot):\n",
    "    for name, history in results_baseline.items():\n",
    "        epochs = [h['epoch'] for h in history]\n",
    "        values = [h[metric] for h in history]\n",
    "        ax.plot(epochs, values, label=name, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(title)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log scale for GNS\n",
    "    if metric == 'grad_noise_scale':\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logs/baseline_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal validation accuracies:\")\n",
    "for name, history in results_baseline.items():\n",
    "    print(f\"  {name}: {history[-1]['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Inner Solver Comparison (ResNet-18)\n",
    "\n",
    "Compare all inner solvers on ResNet-18 for a more realistic benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Inner solver comparison on ResNet-18\n",
    "EPOCHS = 20  # Moderate run\n",
    "\n",
    "results_solvers = {}\n",
    "\n",
    "solver_configs = [\n",
    "    ('SGD (baseline)', 'sgd', 'none'),\n",
    "    ('SpectralClip', 'muon_sgd', 'spectral_clip'),\n",
    "    ('DualAscent', 'muon_sgd', 'dual_ascent'),\n",
    "    ('QuasiNewton', 'muon_sgd', 'quasi_newton'),\n",
    "    ('FrankWolfe', 'muon_sgd', 'frank_wolfe'),\n",
    "    ('ADMM', 'muon_sgd', 'admm'),\n",
    "]\n",
    "\n",
    "for name, opt, solver in solver_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        history, _ = run_experiment(\n",
    "            model_name='resnet18',\n",
    "            optimizer_type=opt,\n",
    "            inner_solver_type=solver,\n",
    "            spectral_budget=0.1,\n",
    "            lr=0.1,\n",
    "            epochs=EPOCHS,\n",
    "            seed=42\n",
    "        )\n",
    "        results_solvers[name] = history\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {e}\")\n",
    "\n",
    "print(\"\\nSolver comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solver comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "for ax, (metric, title) in zip(axes.flatten(), metrics_to_plot):\n",
    "    for name, history in results_solvers.items():\n",
    "        epochs = [h['epoch'] for h in history]\n",
    "        values = [h[metric] for h in history]\n",
    "        ax.plot(epochs, values, label=name, linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(f'{title} (ResNet-18)')\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if metric == 'grad_noise_scale':\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logs/solver_comparison_resnet18.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal validation accuracies (ResNet-18):\")\n",
    "for name, history in sorted(results_solvers.items(), key=lambda x: -x[1][-1]['val_acc']):\n",
    "    print(f\"  {name:20s}: {history[-1]['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Spectral Budget Sweep\n",
    "\n",
    "How does the spectral budget affect training dynamics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Spectral budget sweep\n",
    "EPOCHS = 15\n",
    "\n",
    "budgets = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "results_budget = {}\n",
    "\n",
    "for budget in budgets:\n",
    "    print(f\"\\nRunning with spectral_budget = {budget}...\")\n",
    "    \n",
    "    history, _ = run_experiment(\n",
    "        model_name='small_cnn',\n",
    "        optimizer_type='muon_sgd',\n",
    "        inner_solver_type='spectral_clip',\n",
    "        spectral_budget=budget,\n",
    "        lr=0.1,\n",
    "        epochs=EPOCHS,\n",
    "        seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    results_budget[budget] = history\n",
    "    print(f\"  Final val_acc: {history[-1]['val_acc']:.4f}, max_spec: {history[-1]['max_spectral_norm']:.3f}\")\n",
    "\n",
    "print(\"\\nBudget sweep complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot budget sweep results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Color map for budgets\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(budgets)))\n",
    "\n",
    "# Val accuracy\n",
    "for (budget, history), color in zip(results_budget.items(), colors):\n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    values = [h['val_acc'] for h in history]\n",
    "    axes[0].plot(epochs, values, label=f'η={budget}', color=color, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Accuracy')\n",
    "axes[0].set_title('Accuracy vs Spectral Budget')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Spectral norm\n",
    "for (budget, history), color in zip(results_budget.items(), colors):\n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    values = [h['max_spectral_norm'] for h in history]\n",
    "    axes[1].plot(epochs, values, label=f'η={budget}', color=color, linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Max Spectral Norm')\n",
    "axes[1].set_title('Spectral Norm Trajectory')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Final accuracy vs budget\n",
    "final_accs = [results_budget[b][-1]['val_acc'] for b in budgets]\n",
    "axes[2].plot(budgets, final_accs, 'o-', markersize=10, linewidth=2)\n",
    "axes[2].set_xlabel('Spectral Budget (η)')\n",
    "axes[2].set_ylabel('Final Validation Accuracy')\n",
    "axes[2].set_title('Accuracy vs Budget')\n",
    "axes[2].set_xscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logs/budget_sweep.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: Width Transfer (muP-style)\n",
    "\n",
    "Test if hyperparameters transfer across model widths when using spectral constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Width transfer\n",
    "EPOCHS = 20\n",
    "\n",
    "width_mults = [0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "results_width_muon = {}\n",
    "results_width_sgd = {}\n",
    "\n",
    "print(\"Testing width transfer with MuonSGD...\")\n",
    "for width in width_mults:\n",
    "    print(f\"\\n  Width = {width}x\")\n",
    "    history, _ = run_experiment(\n",
    "        model_name='mlp',\n",
    "        optimizer_type='muon_sgd',\n",
    "        inner_solver_type='spectral_clip',\n",
    "        spectral_budget=0.1,\n",
    "        lr=0.1,\n",
    "        epochs=EPOCHS,\n",
    "        width_mult=width,\n",
    "        seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    results_width_muon[width] = history\n",
    "    print(f\"    Final val_acc: {history[-1]['val_acc']:.4f}\")\n",
    "\n",
    "print(\"\\nTesting width transfer with SGD (baseline)...\")\n",
    "for width in width_mults:\n",
    "    print(f\"\\n  Width = {width}x\")\n",
    "    history, _ = run_experiment(\n",
    "        model_name='mlp',\n",
    "        optimizer_type='sgd',\n",
    "        inner_solver_type='none',\n",
    "        spectral_budget=0.1,\n",
    "        lr=0.1,\n",
    "        epochs=EPOCHS,\n",
    "        width_mult=width,\n",
    "        seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    results_width_sgd[width] = history\n",
    "    print(f\"    Final val_acc: {history[-1]['val_acc']:.4f}\")\n",
    "\n",
    "print(\"\\nWidth transfer experiments complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot width transfer results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Final accuracy vs width\n",
    "muon_accs = [results_width_muon[w][-1]['val_acc'] for w in width_mults]\n",
    "sgd_accs = [results_width_sgd[w][-1]['val_acc'] for w in width_mults]\n",
    "\n",
    "axes[0].plot(width_mults, muon_accs, 'o-', label='MuonSGD', markersize=10, linewidth=2)\n",
    "axes[0].plot(width_mults, sgd_accs, 's--', label='SGD', markersize=10, linewidth=2)\n",
    "axes[0].set_xlabel('Width Multiplier')\n",
    "axes[0].set_ylabel('Final Validation Accuracy')\n",
    "axes[0].set_title('Width Transfer: Accuracy vs Width')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance across widths\n",
    "muon_std = np.std(muon_accs)\n",
    "sgd_std = np.std(sgd_accs)\n",
    "\n",
    "axes[1].bar(['MuonSGD', 'SGD'], [muon_std, sgd_std], color=['tab:blue', 'tab:orange'])\n",
    "axes[1].set_ylabel('Std Dev of Accuracy Across Widths')\n",
    "axes[1].set_title('Transfer Stability (lower = better)')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logs/width_transfer.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTransfer stability (std dev across widths):\")\n",
    "print(f\"  MuonSGD: {muon_std:.4f}\")\n",
    "print(f\"  SGD:     {sgd_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5: LR Stability Envelope\n",
    "\n",
    "Find the maximum stable learning rate for different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: LR stability scan\n",
    "from muon import lr_stability_scan\n",
    "\n",
    "def model_factory():\n",
    "    return get_model('small_cnn')\n",
    "\n",
    "def sgd_factory(model, lr):\n",
    "    return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "def muon_factory(model, lr):\n",
    "    return MuonSGD(\n",
    "        model.parameters(), lr=lr, momentum=0.9,\n",
    "        spectral_budget=0.1, inner_solver=SpectralClipSolver()\n",
    "    )\n",
    "\n",
    "print(\"Scanning LR envelope for SGD...\")\n",
    "sgd_envelope = lr_stability_scan(\n",
    "    model_factory, sgd_factory, ce_loss_fn, train_loader,\n",
    "    lr_range=(1e-3, 2.0), num_lrs=12, steps_per_lr=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nScanning LR envelope for MuonSGD...\")\n",
    "muon_envelope = lr_stability_scan(\n",
    "    model_factory, muon_factory, ce_loss_fn, train_loader,\n",
    "    lr_range=(1e-3, 2.0), num_lrs=12, steps_per_lr=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nMax stable LR:\")\n",
    "print(f\"  SGD:     {sgd_envelope['max_stable_lr']:.4f}\")\n",
    "print(f\"  MuonSGD: {muon_envelope['max_stable_lr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LR envelope\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# SGD\n",
    "sgd_conv_lr = [lr for lr, c in zip(sgd_envelope['lr_values'], sgd_envelope['converged']) if c]\n",
    "sgd_conv_loss = [loss for loss, c in zip(sgd_envelope['final_losses'], sgd_envelope['converged']) if c]\n",
    "sgd_div_lr = [lr for lr, c in zip(sgd_envelope['lr_values'], sgd_envelope['converged']) if not c]\n",
    "\n",
    "ax.scatter(sgd_conv_lr, sgd_conv_loss, c='blue', label='SGD (converged)', s=80, marker='o')\n",
    "ax.scatter(sgd_div_lr, [max(sgd_conv_loss) if sgd_conv_loss else 10] * len(sgd_div_lr),\n",
    "           c='blue', marker='x', s=80, label='SGD (diverged)')\n",
    "\n",
    "# Muon\n",
    "muon_conv_lr = [lr for lr, c in zip(muon_envelope['lr_values'], muon_envelope['converged']) if c]\n",
    "muon_conv_loss = [loss for loss, c in zip(muon_envelope['final_losses'], muon_envelope['converged']) if c]\n",
    "muon_div_lr = [lr for lr, c in zip(muon_envelope['lr_values'], muon_envelope['converged']) if not c]\n",
    "\n",
    "ax.scatter(muon_conv_lr, muon_conv_loss, c='red', label='MuonSGD (converged)', s=80, marker='o')\n",
    "ax.scatter(muon_div_lr, [max(muon_conv_loss) if muon_conv_loss else 10] * len(muon_div_lr),\n",
    "           c='red', marker='x', s=80, label='MuonSGD (diverged)')\n",
    "\n",
    "# Max stable LR lines\n",
    "ax.axvline(sgd_envelope['max_stable_lr'], color='blue', linestyle='--', alpha=0.7,\n",
    "           label=f\"SGD max stable: {sgd_envelope['max_stable_lr']:.3f}\")\n",
    "ax.axvline(muon_envelope['max_stable_lr'], color='red', linestyle='--', alpha=0.7,\n",
    "           label=f\"Muon max stable: {muon_envelope['max_stable_lr']:.3f}\")\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Final Loss (after 100 steps)')\n",
    "ax.set_title('Learning Rate Stability Envelope')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logs/lr_envelope.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to JSON for later analysis\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "all_results = {\n",
    "    'baseline_comparison': {k: v for k, v in results_baseline.items()},\n",
    "    'solver_comparison': {k: v for k, v in results_solvers.items()},\n",
    "    'budget_sweep': {str(k): v for k, v in results_budget.items()},\n",
    "    'width_transfer_muon': {str(k): v for k, v in results_width_muon.items()},\n",
    "    'width_transfer_sgd': {str(k): v for k, v in results_width_sgd.items()},\n",
    "    'lr_envelope_sgd': sgd_envelope,\n",
    "    'lr_envelope_muon': muon_envelope,\n",
    "}\n",
    "\n",
    "with open('logs/all_experiment_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"All results saved to logs/all_experiment_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Key findings from these experiments:\n",
    "\n",
    "1. **Baseline Comparison**: MuonSGD with spectral constraints achieves competitive accuracy while controlling spectral norms\n",
    "\n",
    "2. **Inner Solver Comparison**: Different solvers show trade-offs between computational cost and constraint satisfaction\n",
    "\n",
    "3. **Spectral Budget**: There's an optimal budget range - too small constrains learning, too large doesn't help\n",
    "\n",
    "4. **Width Transfer**: Spectral constraints improve hyperparameter transfer across widths\n",
    "\n",
    "5. **LR Stability**: MuonSGD may extend the stable LR region compared to vanilla SGD\n",
    "\n",
    "---\n",
    "\n",
    "Continue to **03_analysis.ipynb** for deeper analysis of these results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
