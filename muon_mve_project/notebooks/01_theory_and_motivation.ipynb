{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Muon: Theory and Mathematical Foundations\n",
    "\n",
    "**EECS 182 Final Project - Category 1: Optimizers & Hyperparameter Transfer**\n",
    "\n",
    "This notebook provides the theoretical background for our project on spectral-norm constrained optimization and manifold Muon.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Motivation: Why Control Spectral Norms?](#1-motivation)\n",
    "2. [The Muon Inner Problem](#2-muon-inner)\n",
    "3. [Inner Solver Algorithms](#3-inner-solvers)\n",
    "4. [Stability Metrics and Diagnostics](#4-stability-metrics)\n",
    "5. [muP and Hyperparameter Transfer](#5-mup)\n",
    "6. [Project Codebase Overview](#6-codebase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Motivation: Why Control Spectral Norms? <a name=\"1-motivation\"></a>\n",
    "\n",
    "### The Problem with Unconstrained Weight Dynamics\n",
    "\n",
    "In deep neural network training, three types of tensors circulate: **activations**, **gradients**, and **weights**. During optimization, we want to keep their norms stable to avoid:\n",
    "\n",
    "- **Exploding gradients/activations**: $\\|\\mathbf{h}^{(l)}\\| \\to \\infty$ as depth increases\n",
    "- **Vanishing gradients/activations**: $\\|\\mathbf{h}^{(l)}\\| \\to 0$ \n",
    "\n",
    "For a linear layer $\\mathbf{h}^{(l+1)} = W^{(l)} \\mathbf{h}^{(l)}$, the **spectral norm** $\\|W\\|_2 = \\sigma_{\\max}(W)$ directly controls signal amplification:\n",
    "\n",
    "$$\\|W \\mathbf{x}\\|_2 \\leq \\|W\\|_2 \\cdot \\|\\mathbf{x}\\|_2$$\n",
    "\n",
    "If $\\|W^{(l)}\\|_2 > 1$ for many layers, activations explode. If $\\|W^{(l)}\\|_2 < 1$, they vanish.\n",
    "\n",
    "### Prior Approaches\n",
    "\n",
    "| Method | Mechanism | Limitation |\n",
    "|--------|-----------|------------|\n",
    "| **Weight clipping** | Clip $W_{ij} \\in [-c, c]$ | Hyperparameter $c$ is hard to tune |\n",
    "| **Spectral normalization** | Rescale $W \\leftarrow W / \\sigma_{\\max}(W)$ | Doesn't prevent rank collapse |\n",
    "| **Orthogonal regularization** | Soft penalty $\\|W^\\top W - I\\|_F^2$ | Doesn't guarantee constraint satisfaction |\n",
    "\n",
    "### The Manifold Muon Approach\n",
    "\n",
    "**Muon** (MomentUm Orthogonalized by Newton-Schulz) takes a different approach:\n",
    "\n",
    "1. **Constrain the update step**, not just the weights\n",
    "2. **Spectral norm of $\\Delta W$** bounds how much $\\|Wx\\|$ can change for any unit $x$\n",
    "3. Optionally: constrain $W$ to a **manifold** (e.g., Stiefel: $W^\\top W = I$)\n",
    "\n",
    "The key insight: **the step size $\\eta$ directly bounds worst-case output change**:\n",
    "\n",
    "$$\\|(W + \\Delta W)x - Wx\\| = \\|\\Delta W \\cdot x\\| \\leq \\|\\Delta W\\|_2 \\leq \\eta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Muon Inner Problem <a name=\"2-muon-inner\"></a>\n",
    "\n",
    "### Problem Formulation\n",
    "\n",
    "At each optimization step, given gradient $G = \\nabla_W \\mathcal{L}$, we want to find the best update $A$ that:\n",
    "1. Minimizes the linearized loss decrease: $\\langle G, A \\rangle$\n",
    "2. Satisfies the spectral budget: $\\|A\\|_2 \\leq \\eta$\n",
    "3. (Optionally) Stays tangent to the manifold: $A^\\top W + W^\\top A = 0$\n",
    "\n",
    "$$\\boxed{\\min_{A \\in \\mathbb{R}^{m \\times n}} \\text{trace}(G^\\top A) \\quad \\text{s.t.} \\quad \\|A\\|_{\\text{spectral}} \\leq \\eta \\quad \\text{and} \\quad A^\\top W + W^\\top A = 0}$$\n",
    "\n",
    "### Unconstrained Case (No Tangency)\n",
    "\n",
    "Without the tangency constraint, the problem simplifies:\n",
    "\n",
    "$$\\min_{\\|A\\|_2 \\leq \\eta} \\langle G, A \\rangle$$\n",
    "\n",
    "**Closed-form solution**: Let $G = U \\Sigma V^\\top$ be the SVD. The optimal $A^*$ is:\n",
    "\n",
    "$$A^* = -\\eta \\cdot u_1 v_1^\\top$$\n",
    "\n",
    "where $u_1, v_1$ are the top singular vectors of $G$. This is a **rank-1 update** with spectral norm exactly $\\eta$.\n",
    "\n",
    "### Lagrangian Dual Formulation\n",
    "\n",
    "For the constrained case, we form the Lagrangian:\n",
    "\n",
    "$$\\mathcal{L}(A, \\lambda, \\Lambda) = \\langle G, A \\rangle + \\lambda(\\|A\\|_2 - \\eta) + \\langle \\Lambda, A^\\top W + W^\\top A \\rangle$$\n",
    "\n",
    "The dual problem is:\n",
    "\n",
    "$$\\max_{\\lambda \\geq 0, \\Lambda} \\min_A \\mathcal{L}(A, \\lambda, \\Lambda)$$\n",
    "\n",
    "This is what **dual ascent** and **quasi-Newton** methods solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Inner Solver Algorithms <a name=\"3-inner-solvers\"></a>\n",
    "\n",
    "We implement several inner solvers, each with different trade-offs:\n",
    "\n",
    "### 3.1 Spectral Clipping (Baseline)\n",
    "\n",
    "**Simplest approach**: If $\\|\\Delta\\|_2 > \\eta$, rescale:\n",
    "\n",
    "$$\\Delta_{\\text{clipped}} = \\Delta \\cdot \\frac{\\eta}{\\|\\Delta\\|_2}$$\n",
    "\n",
    "**Pros**: Fast, simple  \n",
    "**Cons**: Doesn't respect tangent space, may distort gradient direction\n",
    "\n",
    "### 3.2 Dual Ascent\n",
    "\n",
    "Iterate on the dual variable $\\lambda$:\n",
    "\n",
    "1. For fixed $\\lambda$: compute $A^*(\\lambda)$ in closed form\n",
    "2. Gradient ascent on $\\lambda$: $\\lambda \\leftarrow \\lambda + \\alpha (\\|A^*\\|_2 - \\eta)$\n",
    "\n",
    "**Convergence**: Sublinear $O(1/t)$ for smooth duals\n",
    "\n",
    "### 3.3 Quasi-Newton on the Dual\n",
    "\n",
    "Replace gradient ascent with **L-BFGS** for faster convergence:\n",
    "\n",
    "$$\\lambda_{k+1} = \\lambda_k + H_k^{-1} \\nabla_\\lambda d(\\lambda_k)$$\n",
    "\n",
    "where $H_k$ is approximated from recent $(s, y)$ pairs. Uses **warm-starting** across optimization steps.\n",
    "\n",
    "**Convergence**: Superlinear for well-conditioned problems\n",
    "\n",
    "### 3.4 Frank-Wolfe (Conditional Gradient)\n",
    "\n",
    "**Projection-free** method: never explicitly project onto the constraint set.\n",
    "\n",
    "At iteration $t$:\n",
    "1. **Linear Minimization Oracle (LMO)**: $s_t = \\arg\\min_{\\|A\\|_2 \\leq \\eta} \\langle \\nabla f(A_t), A \\rangle$\n",
    "   - Solution: $s_t = -\\eta \\cdot u_1 v_1^\\top$ (rank-1 atom from top SVD)\n",
    "2. **Update**: $A_{t+1} = (1 - \\gamma_t) A_t + \\gamma_t s_t$\n",
    "\n",
    "**Key property**: $A_t$ is a **convex combination of rank-1 atoms**, so it's **low-rank**!\n",
    "\n",
    "**Convergence**: $O(1/t)$ sublinear, but each step is cheap (just top-SVD)\n",
    "\n",
    "### 3.5 ADMM (Alternating Direction Method of Multipliers)\n",
    "\n",
    "Split the problem into easier subproblems:\n",
    "\n",
    "$$\\min_A \\langle G, A \\rangle \\quad \\text{s.t.} \\quad A = Z, \\quad \\|Z\\|_2 \\leq \\eta$$\n",
    "\n",
    "Augmented Lagrangian:\n",
    "$$L_\\rho(A, Z, U) = \\langle G, A \\rangle + \\frac{\\rho}{2}\\|A - Z + U\\|_F^2 + \\mathbb{I}_{\\|Z\\|_2 \\leq \\eta}$$\n",
    "\n",
    "ADMM iterations:\n",
    "1. **A-update**: $A^{k+1} = Z^k - U^k - G/\\rho$ (closed-form)\n",
    "2. **Z-update**: $Z^{k+1} = \\Pi_{\\|\\cdot\\|_2 \\leq \\eta}(A^{k+1} + U^k)$ (spectral projection)\n",
    "3. **U-update**: $U^{k+1} = U^k + A^{k+1} - Z^{k+1}$\n",
    "\n",
    "**Convergence**: $O(1/t)$ with adaptive $\\rho$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Stability Metrics and Diagnostics <a name=\"4-stability-metrics\"></a>\n",
    "\n",
    "### 4.1 Spectral Norm Tracking\n",
    "\n",
    "We track $\\sigma_{\\max}(W^{(l)})$ for each layer to monitor:\n",
    "- **Lipschitz constant** of each layer\n",
    "- **Conditioning**: $\\kappa(W) = \\sigma_{\\max} / \\sigma_{\\min}$\n",
    "- **Rank collapse**: if small singular values vanish\n",
    "\n",
    "### 4.2 Sharpness (SAM Proxy)\n",
    "\n",
    "Sharpness measures how much the loss increases under small perturbations:\n",
    "\n",
    "$$\\text{Sharpness} \\approx \\mathcal{L}(w + \\epsilon \\cdot g/\\|g\\|) - \\mathcal{L}(w)$$\n",
    "\n",
    "**Connection to Hessian**: For small $\\epsilon$, sharpness $\\approx \\frac{\\epsilon^2}{2} \\lambda_{\\max}(H)$\n",
    "\n",
    "Lower sharpness → flatter minima → better generalization (SAM hypothesis)\n",
    "\n",
    "### 4.3 Gradient Noise Scale (GNS)\n",
    "\n",
    "The **critical batch size** (CBS) determines when increasing batch size stops helping:\n",
    "\n",
    "$$\\text{GNS} = \\frac{\\text{tr}(\\Sigma)}{\\|\\mu\\|^2}$$\n",
    "\n",
    "where $\\mu$ is the true gradient and $\\Sigma$ is the gradient covariance.\n",
    "\n",
    "**Our proxy**: Using two minibatches $g_1, g_2$:\n",
    "\n",
    "$$\\widehat{\\text{GNS}} = \\frac{\\|g_1 - g_2\\|^2}{2}$$\n",
    "\n",
    "### 4.4 Top Hessian Eigenvalue\n",
    "\n",
    "We estimate $\\lambda_{\\max}(H)$ via **power iteration** with Hessian-vector products:\n",
    "\n",
    "1. Initialize random $v$\n",
    "2. Repeat: $v \\leftarrow Hv / \\|Hv\\|$\n",
    "3. Estimate: $\\lambda_{\\max} \\approx v^\\top H v$\n",
    "\n",
    "**HVP computation** (Pearlmutter trick): $Hv = \\nabla_w (\\nabla_w \\mathcal{L} \\cdot v)$\n",
    "\n",
    "### 4.5 Subspace Drift\n",
    "\n",
    "Track how the **principal subspaces** of weight matrices change over training:\n",
    "\n",
    "$$\\text{Drift}(W_{\\text{old}}, W_{\\text{new}}) = \\max_i \\theta_i$$\n",
    "\n",
    "where $\\theta_i$ are the **principal angles** between the top-$k$ singular subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. muP and Hyperparameter Transfer <a name=\"5-mup\"></a>\n",
    "\n",
    "### The Width Transfer Problem\n",
    "\n",
    "Typically, optimal hyperparameters (learning rate, weight decay, etc.) change when we scale model width:\n",
    "\n",
    "$$\\text{Optimal LR for width } w \\neq \\text{Optimal LR for width } 2w$$\n",
    "\n",
    "This makes hyperparameter tuning expensive for large models.\n",
    "\n",
    "### muP (Maximal Update Parameterization)\n",
    "\n",
    "**muP** (Yang & Hu, 2021) derives scaling rules so that:\n",
    "\n",
    "$$\\text{Optimal LR for width } w = \\text{Optimal LR for any width}$$\n",
    "\n",
    "Key insight: scale initialization, learning rates, and activations appropriately with width.\n",
    "\n",
    "### Spectral Constraints and Transfer\n",
    "\n",
    "**Our hypothesis**: Spectral-norm constraints provide an alternative route to hyperparameter transfer:\n",
    "\n",
    "1. The spectral budget $\\eta$ has a **width-independent interpretation**: max output change per step\n",
    "2. If we set $\\eta$ based on desired output perturbation, it should transfer across widths\n",
    "3. This complements muP's initialization-based approach\n",
    "\n",
    "### Experiment Design\n",
    "\n",
    "We test this by:\n",
    "1. Training MLPs at widths $w \\in \\{0.5, 0.75, 1.0, 1.5, 2.0\\} \\times$ base\n",
    "2. Fixing all hyperparameters (LR, spectral budget)\n",
    "3. Comparing final accuracy across widths\n",
    "\n",
    "**If spectral constraints help**: accuracy should be stable across widths  \n",
    "**If not**: accuracy will degrade as width increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Project Codebase Overview <a name=\"6-codebase\"></a>\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "```\n",
    "muon_mve_project/\n",
    "├── muon/\n",
    "│   ├── __init__.py           # Module exports\n",
    "│   ├── inner_solvers.py      # SpectralClip, FrankWolfe, DualAscent, QuasiNewton, ADMM\n",
    "│   ├── muon_sgd.py           # MuonSGD, MuonAdamW optimizers\n",
    "│   └── metrics.py            # Spectral norms, sharpness, GNS, Hessian eigenvalues\n",
    "├── models/\n",
    "│   └── __init__.py           # SmallCNN, ResNet18, TinyViT, MLPMixer, WidthScalableMLP\n",
    "├── scripts/\n",
    "│   ├── run_experiments.py    # Batch experiment runners\n",
    "│   └── aggregate_results.py  # Multi-seed aggregation\n",
    "├── notebooks/\n",
    "│   ├── 01_theory_and_motivation.ipynb  # This notebook\n",
    "│   ├── 02_experiments.ipynb            # Running experiments\n",
    "│   └── 03_analysis.ipynb               # Results analysis\n",
    "├── train.py                  # Main training script\n",
    "├── plot_logs.py              # Plotting utilities\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| `MuonSGD` | SGD with pluggable inner solver for spectral constraints |\n",
    "| `MuonAdamW` | AdamW variant with spectral constraints |\n",
    "| `SpectralClipSolver` | Simple rescaling baseline |\n",
    "| `DualAscentSolver` | Lagrangian dual ascent with warm-starting |\n",
    "| `QuasiNewtonDualSolver` | L-BFGS on the dual |\n",
    "| `FrankWolfeSolver` | Projection-free, low-rank atoms |\n",
    "| `ADMMSolver` | Splitting method with adaptive ρ |\n",
    "\n",
    "### Usage Pattern\n",
    "\n",
    "```python\n",
    "from muon import MuonSGD, get_inner_solver\n",
    "from models import get_model\n",
    "\n",
    "# Create model\n",
    "model = get_model('resnet18', width_mult=1.0)\n",
    "\n",
    "# Create optimizer with spectral constraints\n",
    "solver = get_inner_solver('dual_ascent', max_iters=20)\n",
    "optimizer = MuonSGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    spectral_budget=0.1,\n",
    "    inner_solver=solver\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports work\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from muon import (\n",
    "    MuonSGD, MuonAdamW,\n",
    "    SpectralClipSolver, FrankWolfeSolver, DualAscentSolver,\n",
    "    QuasiNewtonDualSolver, ADMMSolver,\n",
    "    compute_spectral_norms, estimate_sharpness\n",
    ")\n",
    "from models import get_model, MODEL_REGISTRY\n",
    "\n",
    "print(\"Available models:\", list(MODEL_REGISTRY.keys()))\n",
    "print(\"\\nImports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **02_experiments.ipynb** to run the actual experiments, or **03_analysis.ipynb** to analyze existing results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
