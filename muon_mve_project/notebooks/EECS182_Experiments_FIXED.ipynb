{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EECS 182 Manifold Muon Experiments - FIXED\n",
        "\n",
        "**Category 1: Optimizers & Hyperparameter Transfer**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Clone Repository and Setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone the repo (run this once)\n",
        "if not os.path.exists('/content/cs182'):\n",
        "    !git clone https://github.com/achiii800/cs182.git /content/cs182\n",
        "else:\n",
        "    print(\"Repository already exists\")\n",
        "\n",
        "PROJECT_ROOT = '/content/cs182/muon_mve_project'\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"Files: {os.listdir('.')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies\n",
        "!pip install torch torchvision numpy matplotlib scipy pandas tqdm einops -q\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Pre-download CIFAR-10 Dataset\n",
        "# This ensures data is available before training\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "print(\"Downloading CIFAR-10 dataset...\")\n",
        "data_dir = os.path.join(PROJECT_ROOT, 'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download train set\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=data_dir, train=True, download=True,\n",
        "    transform=T.ToTensor()\n",
        ")\n",
        "# Download test set\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=data_dir, train=False, download=True,\n",
        "    transform=T.ToTensor()\n",
        ")\n",
        "\n",
        "print(f\"✓ Train set: {len(train_set)} samples\")\n",
        "print(f\"✓ Test set: {len(test_set)} samples\")\n",
        "print(f\"✓ Data saved to: {data_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: DEBUG - Test imports and find the actual error\n",
        "import os\n",
        "import sys\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "print(\"Testing imports...\")\n",
        "try:\n",
        "    from muon import (\n",
        "        MuonSGD, create_optimizer,\n",
        "        SpectralClipSolver, DualAscentSolver,\n",
        "        compute_spectral_norms, estimate_sharpness, estimate_gradient_noise_scale,\n",
        "        MetricsLogger\n",
        "    )\n",
        "    print(\"✓ muon imports OK\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ muon import error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "try:\n",
        "    from models import get_model, SmallConvNet\n",
        "    print(\"✓ models imports OK\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ models import error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Test creating a model\n",
        "try:\n",
        "    model = get_model('small_cnn', num_classes=10)\n",
        "    print(f\"✓ Model created: {sum(p.numel() for p in model.parameters()):,} params\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Model creation error: {e}\")\n",
        "\n",
        "# Test creating optimizer\n",
        "try:\n",
        "    opt = create_optimizer(model, 'muon_sgd', 'dual_ascent', lr=0.01, spectral_budget=0.1)\n",
        "    print(f\"✓ Optimizer created: {type(opt).__name__}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Optimizer creation error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: DEBUG - Run train.py with visible error output\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Run with error capture\n",
        "cmd = [\n",
        "    sys.executable, 'train.py',\n",
        "    '--model', 'small_cnn',\n",
        "    '--optimizer', 'muon_sgd',\n",
        "    '--inner-solver', 'dual_ascent',\n",
        "    '--spectral-budget', '0.1',\n",
        "    '--lr', '0.01',\n",
        "    '--epochs', '2',  # Just 2 epochs for testing\n",
        "    '--logdir', 'logs/debug',\n",
        "    '--exp-name', 'debug_test',\n",
        "    '--num-workers', '0',  # IMPORTANT: 0 workers for Colab!\n",
        "]\n",
        "\n",
        "print(\"Running debug test...\")\n",
        "print(f\"Command: {' '.join(cmd)}\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "print(\"STDOUT:\")\n",
        "print(result.stdout)\n",
        "\n",
        "if result.stderr:\n",
        "    print(\"\\nSTDERR:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "print(f\"\\nExit code: {result.returncode}\")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n✓ SUCCESS!\")\n",
        "    # Check log file\n",
        "    log_path = os.path.join(PROJECT_ROOT, 'logs/debug/debug_test.csv')\n",
        "    if os.path.exists(log_path):\n",
        "        with open(log_path) as f:\n",
        "            print(f\"\\nLog contents:\\n{f.read()}\")\n",
        "else:\n",
        "    print(\"\\n✗ FAILED - see error above\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: INLINE TRAINING (bypass subprocess entirely)\n",
        "# If Cell 5 still fails, this runs training directly in the notebook\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "from muon import create_optimizer, compute_spectral_norms, estimate_sharpness, estimate_gradient_noise_scale\n",
        "from models import get_model\n",
        "\n",
        "# Config\n",
        "CONFIG = {\n",
        "    'model': 'small_cnn',\n",
        "    'optimizer': 'muon_sgd',\n",
        "    'inner_solver': 'dual_ascent',\n",
        "    'spectral_budget': 0.1,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 5,\n",
        "    'batch_size': 128,\n",
        "    'seed': 42,\n",
        "}\n",
        "\n",
        "print(f\"Config: {CONFIG}\")\n",
        "\n",
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "torch.manual_seed(CONFIG['seed'])\n",
        "\n",
        "# Data\n",
        "transform_train = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "transform_test = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "data_dir = os.path.join(PROJECT_ROOT, 'data')\n",
        "train_set = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n",
        "test_set = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n",
        "\n",
        "# IMPORTANT: num_workers=0 for Colab!\n",
        "train_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Model\n",
        "model = get_model(CONFIG['model'], num_classes=10).to(device)\n",
        "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = create_optimizer(\n",
        "    model,\n",
        "    optimizer_type=CONFIG['optimizer'],\n",
        "    inner_solver_type=CONFIG['inner_solver'],\n",
        "    lr=CONFIG['lr'],\n",
        "    spectral_budget=CONFIG['spectral_budget'],\n",
        ")\n",
        "print(f\"Optimizer: {type(optimizer).__name__}\")\n",
        "\n",
        "# Loss function for metrics\n",
        "def ce_loss_fn(model, x, y):\n",
        "    return F.cross_entropy(model(x), y)\n",
        "\n",
        "# Training loop\n",
        "results = []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for epoch in range(1, CONFIG['epochs'] + 1):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        running_correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    \n",
        "    train_loss = running_loss / total\n",
        "    train_acc = running_correct / total\n",
        "    \n",
        "    # Eval\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            val_loss += F.cross_entropy(logits, y, reduction='sum').item()\n",
        "            val_correct += (logits.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "    \n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "    \n",
        "    # Metrics\n",
        "    spec_norms = compute_spectral_norms(model, max_layers=8)\n",
        "    max_spec = max(spec_norms.values()) if spec_norms else 0.0\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    results.append({\n",
        "        'epoch': epoch,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'max_spectral_norm': max_spec,\n",
        "        'time': epoch_time,\n",
        "    })\n",
        "    \n",
        "    print(f\"Epoch {epoch:2d} | Train: {train_loss:.4f}/{train_acc:.4f} | Val: {val_loss:.4f}/{val_acc:.4f} | σ_max: {max_spec:.3f} | {epoch_time:.1f}s\")\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(f\"Final val accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "# Save results\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "os.makedirs('logs/inline_test', exist_ok=True)\n",
        "df.to_csv('logs/inline_test/results.csv', index=False)\n",
        "print(f\"\\n✓ Results saved to logs/inline_test/results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment 1: Inner Solver Comparison (Fixed)\n",
        "\n",
        "Now that we know the inline training works, let's run the full experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Experiment 1 - Inner Solver Comparison (FIXED)\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "from muon import create_optimizer, compute_spectral_norms, estimate_sharpness, estimate_gradient_noise_scale\n",
        "from models import get_model\n",
        "\n",
        "# Config\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.01\n",
        "SPECTRAL_BUDGET = 1.0\n",
        "SEED = 42\n",
        "\n",
        "solvers = ['spectral_clip', 'dual_ascent', 'quasi_newton', 'frank_wolfe', 'admm']\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Data (load once)\n",
        "transform_train = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "transform_test = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "data_dir = os.path.join(PROJECT_ROOT, 'data')\n",
        "train_set = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n",
        "test_set = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Results storage\n",
        "all_results = {}\n",
        "log_dir = os.path.join(PROJECT_ROOT, 'logs', 'exp1_solver_comparison')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EXPERIMENT 1: Inner Solver Comparison\")\n",
        "print(f\"Epochs: {EPOCHS}, LR: {LR}, Budget: {SPECTRAL_BUDGET}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def ce_loss_fn(model, x, y):\n",
        "    return F.cross_entropy(model(x), y)\n",
        "\n",
        "for solver_name in solvers:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Solver: {solver_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "    \n",
        "    # Fresh model\n",
        "    model = get_model('small_cnn', num_classes=10).to(device)\n",
        "    \n",
        "    # Optimizer with this solver\n",
        "    optimizer = create_optimizer(\n",
        "        model,\n",
        "        optimizer_type='muon_sgd',\n",
        "        inner_solver_type=solver_name,\n",
        "        lr=LR,\n",
        "        spectral_budget=SPECTRAL_BUDGET,\n",
        "    )\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # Train\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        # Get batches for GNS\n",
        "        gns_batches = []\n",
        "        \n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            \n",
        "            if len(gns_batches) < 2:\n",
        "                gns_batches.append((x.clone(), y.clone()))\n",
        "            \n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            running_correct += (logits.argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "        \n",
        "        train_loss = running_loss / total\n",
        "        train_acc = running_correct / total\n",
        "        \n",
        "        # Eval\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                val_loss += F.cross_entropy(logits, y, reduction='sum').item()\n",
        "                val_correct += (logits.argmax(1) == y).sum().item()\n",
        "                val_total += y.size(0)\n",
        "        \n",
        "        val_loss /= val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        \n",
        "        # Metrics\n",
        "        spec_norms = compute_spectral_norms(model, max_layers=8)\n",
        "        max_spec = max(spec_norms.values()) if spec_norms else 0.0\n",
        "        \n",
        "        # Sharpness (every epoch)\n",
        "        try:\n",
        "            sharpness = estimate_sharpness(model, ce_loss_fn, gns_batches[0][0], gns_batches[0][1], epsilon=1e-3)\n",
        "        except:\n",
        "            sharpness = 0.0\n",
        "        \n",
        "        # GNS (every epoch)\n",
        "        try:\n",
        "            if len(gns_batches) >= 2:\n",
        "                gns = estimate_gradient_noise_scale(model, ce_loss_fn, gns_batches[0], gns_batches[1])\n",
        "            else:\n",
        "                gns = 0.0\n",
        "        except:\n",
        "            gns = 0.0\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        \n",
        "        results.append({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'max_spectral_norm': max_spec,\n",
        "            'sharpness': sharpness,\n",
        "            'gns': gns,\n",
        "        })\n",
        "        \n",
        "        print(f\"Epoch {epoch:2d} | Train: {train_loss:.4f}/{train_acc:.4f} | Val: {val_loss:.4f}/{val_acc:.4f} | σ: {max_spec:.3f} | {epoch_time:.1f}s\")\n",
        "    \n",
        "    # Save\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(log_dir, f'solver_{solver_name}.csv')\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    all_results[solver_name] = df\n",
        "    print(f\"✓ Saved to {csv_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT 1 COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Plot Experiment 1 Results\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "log_dir = os.path.join(PROJECT_ROOT, 'logs', 'exp1_solver_comparison')\n",
        "\n",
        "solvers = ['spectral_clip', 'dual_ascent', 'quasi_newton', 'frank_wolfe', 'admm']\n",
        "colors = {\n",
        "    'spectral_clip': '#1f77b4',\n",
        "    'dual_ascent': '#ff7f0e',\n",
        "    'quasi_newton': '#2ca02c',\n",
        "    'frank_wolfe': '#d62728',\n",
        "    'admm': '#9467bd'\n",
        "}\n",
        "\n",
        "# Load data\n",
        "data = {}\n",
        "for solver in solvers:\n",
        "    path = os.path.join(log_dir, f'solver_{solver}.csv')\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        if len(df) > 0:\n",
        "            data[solver] = df\n",
        "            print(f\"✓ {solver}: {len(df)} epochs, final val_acc={df['val_acc'].iloc[-1]*100:.2f}%\")\n",
        "\n",
        "if len(data) == 0:\n",
        "    print(\"No data found!\")\n",
        "else:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    for solver, df in data.items():\n",
        "        c = colors[solver]\n",
        "        axes[0,0].plot(df['epoch'], df['train_loss'], label=solver, color=c, lw=2)\n",
        "        axes[0,1].plot(df['epoch'], df['val_acc']*100, label=solver, color=c, lw=2)\n",
        "        axes[1,0].plot(df['epoch'], df['max_spectral_norm'], label=solver, color=c, lw=2)\n",
        "        axes[1,1].plot(df['epoch'], df['sharpness'], label=solver, color=c, lw=2)\n",
        "    \n",
        "    axes[0,0].set_xlabel('Epoch'); axes[0,0].set_ylabel('Train Loss'); axes[0,0].set_title('Training Loss')\n",
        "    axes[0,1].set_xlabel('Epoch'); axes[0,1].set_ylabel('Val Acc (%)'); axes[0,1].set_title('Validation Accuracy')\n",
        "    axes[1,0].set_xlabel('Epoch'); axes[1,0].set_ylabel('σ_max'); axes[1,0].set_title('Max Spectral Norm')\n",
        "    axes[1,1].set_xlabel('Epoch'); axes[1,1].set_ylabel('Sharpness'); axes[1,1].set_title('Sharpness (SAM proxy)')\n",
        "    \n",
        "    for ax in axes.flat:\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    fig_path = os.path.join(log_dir, 'exp1_results.png')\n",
        "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\n✓ Figure saved to {fig_path}\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Summary table\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Solver':<15} {'Val Acc':<12} {'Train Loss':<12} {'σ_max':<10}\")\n",
        "    print(\"-\"*60)\n",
        "    for solver, df in sorted(data.items(), key=lambda x: -x[1]['val_acc'].iloc[-1]):\n",
        "        print(f\"{solver:<15} {df['val_acc'].iloc[-1]*100:>10.2f}% {df['train_loss'].iloc[-1]:>12.4f} {df['max_spectral_norm'].iloc[-1]:>10.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment 2: Multi-Seed Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Experiment 2 - Multi-Seed Baseline Comparison on ResNet-18\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "from muon import create_optimizer, compute_spectral_norms\n",
        "from models import get_model\n",
        "\n",
        "# Config\n",
        "EPOCHS = 50  # Full training\n",
        "BATCH_SIZE = 128\n",
        "SEEDS = [0, 1, 2]\n",
        "\n",
        "configs = [\n",
        "    # (name, optimizer, inner_solver, lr, spectral_budget)\n",
        "    ('sgd', 'sgd', 'none', 0.1, None),\n",
        "    ('adamw', 'adamw', 'none', 0.001, None),\n",
        "    ('muon_dual', 'muon_sgd', 'dual_ascent', 0.1, 0.1),\n",
        "    ('muon_clip', 'muon_sgd', 'spectral_clip', 0.1, 0.1),\n",
        "]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data\n",
        "transform_train = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "transform_test = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "data_dir = os.path.join(PROJECT_ROOT, 'data')\n",
        "train_set = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n",
        "test_set = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "log_dir = os.path.join(PROJECT_ROOT, 'logs', 'exp2_baselines')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXPERIMENT 2: Multi-Seed Baselines on ResNet-18\")\n",
        "print(f\"Epochs: {EPOCHS}, Seeds: {SEEDS}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for config_name, opt_type, solver_type, lr, budget in configs:\n",
        "    for seed in SEEDS:\n",
        "        exp_name = f\"{config_name}_seed{seed}\"\n",
        "        print(f\"\\nRunning: {exp_name}\")\n",
        "        \n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "        model = get_model('resnet18', num_classes=10).to(device)\n",
        "        optimizer = create_optimizer(\n",
        "            model, optimizer_type=opt_type, inner_solver_type=solver_type,\n",
        "            lr=lr, spectral_budget=budget,\n",
        "        )\n",
        "        \n",
        "        # Cosine LR schedule\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        for epoch in range(1, EPOCHS + 1):\n",
        "            epoch_start = time.time()\n",
        "            \n",
        "            # Train\n",
        "            model.train()\n",
        "            running_loss, running_correct, total = 0.0, 0, 0\n",
        "            \n",
        "            for x, y in train_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * x.size(0)\n",
        "                running_correct += (logits.argmax(1) == y).sum().item()\n",
        "                total += y.size(0)\n",
        "            \n",
        "            scheduler.step()\n",
        "            train_loss = running_loss / total\n",
        "            train_acc = running_correct / total\n",
        "            \n",
        "            # Eval\n",
        "            model.eval()\n",
        "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for x, y in test_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    logits = model(x)\n",
        "                    val_loss += F.cross_entropy(logits, y, reduction='sum').item()\n",
        "                    val_correct += (logits.argmax(1) == y).sum().item()\n",
        "                    val_total += y.size(0)\n",
        "            \n",
        "            val_loss /= val_total\n",
        "            val_acc = val_correct / val_total\n",
        "            \n",
        "            spec_norms = compute_spectral_norms(model, max_layers=8)\n",
        "            max_spec = max(spec_norms.values()) if spec_norms else 0.0\n",
        "            \n",
        "            results.append({\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss, 'train_acc': train_acc,\n",
        "                'val_loss': val_loss, 'val_acc': val_acc,\n",
        "                'max_spectral_norm': max_spec,\n",
        "            })\n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"  Epoch {epoch:2d}: val_acc={val_acc*100:.2f}%\")\n",
        "        \n",
        "        df = pd.DataFrame(results)\n",
        "        df.to_csv(os.path.join(log_dir, f'{exp_name}.csv'), index=False)\n",
        "        print(f\"  ✓ Final: {val_acc*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXPERIMENT 2 COMPLETE!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Plot Experiment 2 with Error Bars\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "log_dir = os.path.join(PROJECT_ROOT, 'logs', 'exp2_baselines')\n",
        "\n",
        "configs = ['sgd', 'adamw', 'muon_dual', 'muon_clip']\n",
        "seeds = [0, 1, 2]\n",
        "\n",
        "colors = {'sgd': '#1f77b4', 'adamw': '#ff7f0e', 'muon_dual': '#2ca02c', 'muon_clip': '#d62728'}\n",
        "labels = {'sgd': 'SGD', 'adamw': 'AdamW', 'muon_dual': 'MuonSGD (DualAscent)', 'muon_clip': 'MuonSGD (SpectralClip)'}\n",
        "\n",
        "aggregated = defaultdict(list)\n",
        "\n",
        "for cfg in configs:\n",
        "    for seed in seeds:\n",
        "        path = os.path.join(log_dir, f'{cfg}_seed{seed}.csv')\n",
        "        if os.path.exists(path):\n",
        "            df = pd.read_csv(path)\n",
        "            if len(df) > 0:\n",
        "                aggregated[cfg].append(df)\n",
        "\n",
        "if all(len(v) == 0 for v in aggregated.values()):\n",
        "    print(\"No data found. Run Experiment 2 first.\")\n",
        "else:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    for cfg, dfs in aggregated.items():\n",
        "        if len(dfs) == 0:\n",
        "            continue\n",
        "        \n",
        "        min_len = min(len(df) for df in dfs)\n",
        "        epochs = dfs[0]['epoch'].values[:min_len]\n",
        "        \n",
        "        losses = np.array([df['train_loss'].values[:min_len] for df in dfs])\n",
        "        accs = np.array([df['val_acc'].values[:min_len] * 100 for df in dfs])\n",
        "        \n",
        "        mean_loss, std_loss = losses.mean(0), losses.std(0)\n",
        "        mean_acc, std_acc = accs.mean(0), accs.std(0)\n",
        "        \n",
        "        c = colors[cfg]\n",
        "        lbl = labels[cfg]\n",
        "        \n",
        "        axes[0].plot(epochs, mean_loss, label=lbl, color=c, lw=2)\n",
        "        axes[0].fill_between(epochs, mean_loss-std_loss, mean_loss+std_loss, color=c, alpha=0.2)\n",
        "        \n",
        "        axes[1].plot(epochs, mean_acc, label=lbl, color=c, lw=2)\n",
        "        axes[1].fill_between(epochs, mean_acc-std_acc, mean_acc+std_acc, color=c, alpha=0.2)\n",
        "        \n",
        "        print(f\"{lbl}: {mean_acc[-1]:.2f}% ± {std_acc[-1]:.2f}%\")\n",
        "    \n",
        "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Train Loss')\n",
        "    axes[0].set_title('Training Loss (mean ± std)')\n",
        "    axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Val Acc (%)')\n",
        "    axes[1].set_title('Validation Accuracy (mean ± std)')\n",
        "    axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(log_dir, 'exp2_results.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment 3: Width Transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Experiment 3 - Width Transfer on MLP\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "from muon import create_optimizer, compute_spectral_norms\n",
        "from models import get_model\n",
        "\n",
        "# Config\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 128\n",
        "WIDTHS = [0.5, 0.75, 1.0, 1.5, 2.0]\n",
        "SEEDS = [0, 1, 2]\n",
        "LR = 0.01\n",
        "\n",
        "configs = [\n",
        "    ('sgd', 'sgd', 'none', None),\n",
        "    ('muon_dual', 'muon_sgd', 'dual_ascent', 0.1),\n",
        "]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data\n",
        "transform = T.Compose([T.ToTensor(), T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "data_dir = os.path.join(PROJECT_ROOT, 'data')\n",
        "train_set = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "log_dir = os.path.join(PROJECT_ROOT, 'logs', 'exp3_width_transfer')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXPERIMENT 3: Width Transfer on MLP\")\n",
        "print(f\"Widths: {WIDTHS}, Seeds: {SEEDS}, Epochs: {EPOCHS}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for cfg_name, opt_type, solver_type, budget in configs:\n",
        "    for width in WIDTHS:\n",
        "        for seed in SEEDS:\n",
        "            exp_name = f\"{cfg_name}_w{width}_s{seed}\"\n",
        "            print(f\"Running: {exp_name}\")\n",
        "            \n",
        "            torch.manual_seed(seed)\n",
        "            \n",
        "            model = get_model('mlp', num_classes=10, width_mult=width).to(device)\n",
        "            optimizer = create_optimizer(\n",
        "                model, optimizer_type=opt_type, inner_solver_type=solver_type,\n",
        "                lr=LR, spectral_budget=budget,\n",
        "            )\n",
        "            \n",
        "            results = []\n",
        "            \n",
        "            for epoch in range(1, EPOCHS + 1):\n",
        "                model.train()\n",
        "                for x, y in train_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = F.cross_entropy(model(x), y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "                model.eval()\n",
        "                correct, total = 0, 0\n",
        "                with torch.no_grad():\n",
        "                    for x, y in test_loader:\n",
        "                        x, y = x.to(device), y.to(device)\n",
        "                        correct += (model(x).argmax(1) == y).sum().item()\n",
        "                        total += y.size(0)\n",
        "                val_acc = correct / total\n",
        "                results.append({'epoch': epoch, 'val_acc': val_acc})\n",
        "            \n",
        "            df = pd.DataFrame(results)\n",
        "            df.to_csv(os.path.join(log_dir, f'{exp_name}.csv'), index=False)\n",
        "            print(f\"  Final: {val_acc*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXPERIMENT 3 COMPLETE!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Plot Width Transfer Results\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_dir = os.path.join(PROJECT_ROOT, 'logs', 'exp3_width_transfer')\n",
        "\n",
        "configs = ['sgd', 'muon_dual']\n",
        "widths = [0.5, 0.75, 1.0, 1.5, 2.0]\n",
        "seeds = [0, 1, 2]\n",
        "\n",
        "colors = {'sgd': '#1f77b4', 'muon_dual': '#2ca02c'}\n",
        "labels = {'sgd': 'SGD', 'muon_dual': 'MuonSGD (DualAscent)'}\n",
        "\n",
        "results = {cfg: {'widths': [], 'mean': [], 'std': []} for cfg in configs}\n",
        "\n",
        "for cfg in configs:\n",
        "    for w in widths:\n",
        "        accs = []\n",
        "        for s in seeds:\n",
        "            path = os.path.join(log_dir, f'{cfg}_w{w}_s{s}.csv')\n",
        "            if os.path.exists(path):\n",
        "                df = pd.read_csv(path)\n",
        "                if len(df) > 0:\n",
        "                    accs.append(df['val_acc'].iloc[-1] * 100)\n",
        "        \n",
        "        if accs:\n",
        "            results[cfg]['widths'].append(w)\n",
        "            results[cfg]['mean'].append(np.mean(accs))\n",
        "            results[cfg]['std'].append(np.std(accs))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for cfg in configs:\n",
        "    r = results[cfg]\n",
        "    if r['widths']:\n",
        "        ax.errorbar(r['widths'], r['mean'], yerr=r['std'],\n",
        "                   label=labels[cfg], color=colors[cfg],\n",
        "                   marker='o', markersize=8, lw=2, capsize=5)\n",
        "\n",
        "ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax.set_xlabel('Width Multiplier', fontsize=12)\n",
        "ax.set_ylabel('Final Val Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Width Transfer: MuonSGD vs SGD on MLP', fontsize=14)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(log_dir, 'exp3_width_transfer.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "print(\"\\nWidth Transfer Summary:\")\n",
        "for cfg in configs:\n",
        "    r = results[cfg]\n",
        "    if r['widths']:\n",
        "        print(f\"\\n{labels[cfg]}:\")\n",
        "        for w, m, s in zip(r['widths'], r['mean'], r['std']):\n",
        "            print(f\"  Width {w}: {m:.2f}% ± {s:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
